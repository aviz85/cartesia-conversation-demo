# Cartesia Voice Conversation Demo

A real-time streaming voice conversation application using **Cartesia.ai** for Speech-to-Text (STT) and Text-to-Speech (TTS), powered by **OpenAI GPT-4o-mini** for intelligent responses in Hebrew.

## ğŸŒŸ Features

- **Real-time Speech-to-Text**: Stream audio from your microphone directly to Cartesia's STT API
- **AI-Powered Responses**: GPT-4o-mini processes transcriptions and generates intelligent responses in Hebrew
- **Streaming Text-to-Speech**: AI responses are streamed back as natural-sounding Hebrew voice
- **Full-Duplex Communication**: Everything works in real-time with optimized streaming
- **Comprehensive Latency Tracking**: Monitor performance at every stage with detailed metrics
- **Beautiful UI**: Clean, modern interface with visual feedback, audio visualizer, and performance dashboard

## ğŸ—ï¸ Architecture

### Current Implementation (Node.js + WebSockets)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CLIENT (Browser)                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Audio        â”‚â”€â”€â”€â–¶â”‚ WebSocket    â”‚â”€â”€â”€â–¶â”‚ Audio        â”‚      â”‚
â”‚  â”‚ Capture      â”‚    â”‚ Connection   â”‚    â”‚ Playback     â”‚      â”‚
â”‚  â”‚ (16kHz PCM)  â”‚    â”‚              â”‚    â”‚ (Real-time)  â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚         â”‚                    â”‚                    â–²              â”‚
â”‚         â”‚ Base64 Audio       â”‚ Audio Chunks       â”‚              â”‚
â”‚         â–¼                    â–¼                    â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â”‚ WebSocket (Bidirectional)
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   NODE.JS SERVER (Express + WS)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  WebSocket Handler                                       â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚   â”‚
â”‚  â”‚  â”‚ Receive  â”‚â”€â–¶â”‚ Pipeline â”‚â”€â–¶â”‚ Stream   â”‚             â”‚   â”‚
â”‚  â”‚  â”‚ Audio    â”‚  â”‚ Manager  â”‚  â”‚ Response â”‚             â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚   â”‚
â”‚  â”‚       â”‚              â”‚              â”‚                   â”‚   â”‚
â”‚  â”‚       â”‚ Latency Tracking (7 stages)â”‚                   â”‚   â”‚
â”‚  â”‚       â–¼              â–¼              â–¼                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â”‚              â”‚              â”‚                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚              â”‚              â”‚
          â–¼              â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Cartesia    â”‚  â”‚   OpenAI     â”‚  â”‚  Cartesia    â”‚
â”‚     STT      â”‚  â”‚  GPT-4o-mini â”‚  â”‚     TTS      â”‚
â”‚  (WebSocket) â”‚  â”‚  (Streaming) â”‚  â”‚  (WebSocket) â”‚
â”‚   Hebrew     â”‚  â”‚   Hebrew     â”‚  â”‚ sonic-multi  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Upcoming: Vercel Deployment (Edge Functions + HTTP Streaming)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CLIENT (Vite React App)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Audio        â”‚â”€â”€â”€â–¶â”‚ HTTP POST    â”‚â”€â”€â”€â–¶â”‚ Audio        â”‚      â”‚
â”‚  â”‚ Capture      â”‚    â”‚ + SSE Stream â”‚    â”‚ Playback     â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚         â”‚                    â”‚                    â–²              â”‚
â”‚         â”‚ POST /api/conversation  â”‚ SSE Events    â”‚              â”‚
â”‚         â–¼                    â–¼                    â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â”‚ HTTP (Request + Streaming Response)
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              VERCEL EDGE FUNCTION (Global Edge Network)          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  /api/conversation.ts (Edge Runtime)                     â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚   â”‚
â”‚  â”‚  â”‚ Receive  â”‚â”€â–¶â”‚ Stream   â”‚â”€â–¶â”‚ Stream   â”‚             â”‚   â”‚
â”‚  â”‚  â”‚ Audio    â”‚  â”‚ Pipeline â”‚  â”‚ Response â”‚             â”‚   â”‚
â”‚  â”‚  â”‚ (POST)   â”‚  â”‚          â”‚  â”‚ (SSE)    â”‚             â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚   â”‚
â”‚  â”‚       â”‚              â”‚              â”‚                   â”‚   â”‚
â”‚  â”‚       â”‚  Latency Tracking + Secure API Keys            â”‚   â”‚
â”‚  â”‚       â–¼              â–¼              â–¼                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â”‚              â”‚              â”‚                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚              â”‚              â”‚
          â–¼              â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Cartesia    â”‚  â”‚   OpenAI     â”‚  â”‚  Cartesia    â”‚
â”‚     STT      â”‚  â”‚  GPT-4o-mini â”‚  â”‚     TTS      â”‚
â”‚  (WebSocket) â”‚  â”‚  (Streaming) â”‚  â”‚  (WebSocket) â”‚
â”‚   Hebrew     â”‚  â”‚   Hebrew     â”‚  â”‚ sonic-multi  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ Streaming Solution Explained

### Why Streaming?

Traditional request-response patterns introduce significant latency in voice conversations. Our streaming architecture ensures:
- **Immediate feedback**: Users see transcription as they speak
- **Faster responses**: AI starts speaking before completing the full response
- **Natural conversation flow**: Mimics human-to-human interaction
- **Lower perceived latency**: Progressive rendering keeps users engaged

### Multi-Stage Streaming Pipeline

#### Stage 1: Audio Capture â†’ STT (Speech-to-Text)
```
Microphone â†’ Web Audio API â†’ PCM 16kHz Mono â†’ Cartesia STT WebSocket
                                                     â†“
                                            Streaming Transcript
```
- **Format**: PCM S16LE (16-bit signed little-endian)
- **Sample Rate**: 16kHz (optimal for speech recognition)
- **Language**: Hebrew (`language=he`)
- **Model**: `ink-whisper` (optimized for conversational AI)

#### Stage 2: Transcript â†’ LLM (Large Language Model)
```
Transcript â†’ OpenAI GPT-4o-mini Streaming API â†’ Token-by-token Response
                                                        â†“
                                                   Hebrew Text
```
- **Streaming Mode**: Server-Sent Events (SSE)
- **First Token Latency**: <300ms (tracked)
- **System Prompt**: Hebrew conversation assistant
- **Context**: Full conversation history maintained

#### Stage 3: Text â†’ TTS (Text-to-Speech)
```
Text Chunks â†’ Cartesia TTS Context-based Streaming â†’ Audio Chunks
              (sentence boundaries)                         â†“
                                                      PCM Audio
```
- **Context ID**: Maintains prosody across chunks
- **Continue Flag**: `true` for streaming, `false` for final chunk
- **Model**: `sonic-multilingual` with explicit `language: 'he'`
- **Voice**: Custom Hebrew voice (configurable)
- **First Byte Latency**: <150ms (tracked)

#### Stage 4: Audio Playback
```
Audio Chunks â†’ Web Audio API Buffer â†’ Real-time Playback
```
- **Buffering**: Queue-based with automatic playback
- **Sample Rate**: 16kHz
- **Decoding**: Base64 â†’ PCM â†’ Float32 â†’ AudioBuffer

### Latency Optimization Techniques

1. **Pre-connected WebSockets**: Connections established before first use
2. **Aggressive Chunking**: Stream on all punctuation boundaries (`.!?,;:`)
3. **Context-based TTS**: Maintains natural prosody across chunks
4. **Parallel Processing**: STT, LLM, and TTS overlap when possible
5. **Client-side Buffering**: Smooth audio playback without gaps

## ğŸ“Š Latency Tracking

The application tracks latency at every stage:

| Stage | Description | Target | Tracked Metric |
|-------|-------------|--------|----------------|
| **Recording** | Audio capture duration | N/A | User-controlled |
| **STT** | Speech-to-text processing | <500ms | `sttLatency` |
| **LLM First Token** | Time to first AI response | <300ms | `llmFirstToken` |
| **LLM Total** | Full AI response generation | Variable | `llmLatency` |
| **TTS First Byte** | Time to first audio output | <150ms | `ttsFirstByte` |
| **TTS Total** | Full audio generation | Variable | `ttsLatency` |
| **End-to-End** | Complete turn latency | <2000ms | `e2eLatency` |

### Performance Metrics UI

The application displays real-time latency metrics with:
- **Color-coded indicators**: Green (<3s), Orange (<5s), Red (>5s)
- **Stage breakdown**: See exactly where time is spent
- **Console logging**: Detailed timing information for debugging

## ğŸš€ Vercel Deployment Strategy

### Why Vercel Edge Functions?

| Feature | Traditional Server | Vercel Edge Functions |
|---------|-------------------|----------------------|
| **WebSocket Support** | âœ… Native | âŒ Not supported |
| **HTTP Streaming** | âœ… Supported | âœ… Native support |
| **Global Distribution** | âŒ Single region | âœ… 300+ edge locations |
| **Cold Start** | ~1s | <50ms |
| **API Key Security** | âœ… Server-side | âœ… Server-side |
| **Auto-scaling** | âŒ Manual | âœ… Automatic |
| **Deployment** | Complex | `vercel deploy` |

### Architecture Differences

**Current (Node.js)**:
- Client â†” WebSocket â†” Server â†” APIs
- Bidirectional persistent connection
- Server maintains state per connection

**Vercel (Edge Functions)**:
- Client â†’ HTTP POST â†’ Edge Function â†’ APIs
- Client â† SSE Stream â† Edge Function â† APIs
- Stateless, streaming responses
- Edge Function acts as streaming proxy

### Trade-offs

**Advantages of Vercel:**
- âœ… Lower global latency (edge network)
- âœ… Better scalability (auto-scaling)
- âœ… Simpler deployment (no server management)
- âœ… Cost-effective (pay per execution)
- âœ… Built-in CDN for static assets

**Considerations:**
- âš ï¸ No persistent WebSocket connections (use HTTP streaming)
- âš ï¸ 60s max function duration (sufficient for our use case)
- âš ï¸ Requires refactoring to HTTP-based streaming

### Migration Plan

See `VERCEL_DEPLOYMENT_PLAN.md` for the complete migration strategy including:
- New Vite + React architecture
- Edge Function implementation
- HTTP streaming with SSE
- Performance optimization strategies

## ğŸ› ï¸ Prerequisites

- Node.js 18+ (for ES modules and WebSocket support)
- Cartesia.ai API key ([Get it here](https://cartesia.ai))
- OpenAI API key ([Get it here](https://platform.openai.com))

## ğŸ“¥ Installation

1. **Clone or navigate to the project directory**:
   ```bash
   cd cartesia-conversation-demo
   ```

2. **Install dependencies**:
   ```bash
   npm install
   ```

3. **Configure environment variables**:
   ```bash
   cp .env.example .env
   ```

4. **Edit `.env` and add your API keys**:
   ```env
   # Cartesia API Configuration
   CARTESIA_API_KEY=your_cartesia_api_key_here
   CARTESIA_API_VERSION=2025-04-16

   # Cartesia Voice Configuration
   CARTESIA_VOICE_ID=5351f3f8-06be-4963-800d-fce17daab951
   CARTESIA_MODEL_ID=sonic-multilingual

   # OpenAI API Configuration
   OPENAI_API_KEY=your_openai_api_key_here
   OPENAI_MODEL=gpt-4o-mini

   # Server Configuration
   PORT=3000
   ```

## ğŸ¯ Usage

1. **Start the server**:
   ```bash
   npm start
   ```

   Or for development with auto-reload:
   ```bash
   npm run dev
   ```

2. **Open your browser**:
   Navigate to `http://localhost:3000`

3. **Start a conversation**:
   - Click "Connect" to initialize all connections
   - Click "Start Recording" and speak your message in Hebrew
   - Click "Stop Recording" when done
   - The AI will transcribe your speech, process it, and respond with voice
   - Monitor latency metrics in real-time

## ğŸ”§ Technical Details

### Audio Format

- **Sample Rate**: 16000 Hz (recommended for best performance)
- **Encoding**: PCM S16LE (signed 16-bit little-endian)
- **Channels**: Mono (1 channel)
- **Language**: Hebrew (ISO 639-1: `he`)

### API Endpoints

- **Cartesia STT**: `wss://api.cartesia.ai/stt/stream?language=he&model=ink-whisper`
- **Cartesia TTS**: `wss://api.cartesia.ai/tts/websocket`
- **OpenAI Chat**: `https://api.openai.com/v1/chat/completions` (streaming)

### Streaming Flow

1. **User speaks** â†’ Audio captured via Web Audio API
2. **Audio â†’ STT** â†’ Converted to PCM S16LE and streamed to Cartesia STT WebSocket
3. **STT â†’ LLM** â†’ Transcript forwarded to OpenAI GPT-4o-mini with streaming enabled
4. **LLM â†’ TTS** â†’ Response chunked by sentence boundaries and sent to Cartesia TTS
5. **TTS â†’ Client** â†’ Audio chunks streamed back and played immediately
6. **Latency Tracking** â†’ Every stage timed and displayed in UI

### Context-Based TTS Streaming

Cartesia's context-based streaming maintains natural prosody:

```javascript
// First chunk
{
  context_id: "ctx_12345",
  transcript: "×©×œ×•×,",
  continue: true,  // More text coming
  language: "he",
  voice: { id: "..." }
}

// Middle chunk
{
  context_id: "ctx_12345",
  transcript: "××” ×©×œ×•××š?",
  continue: true,  // Still more
  language: "he",
  voice: { id: "..." }
}

// Final chunk
{
  context_id: "ctx_12345",
  transcript: "××™×š ×× ×™ ×™×›×•×œ ×œ×¢×–×•×¨?",
  continue: false,  // Done
  language: "he",
  voice: { id: "..." }
}
```

This ensures smooth, natural speech across multiple text chunks.

## âš™ï¸ Configuration Options

### Voice Selection

Change the AI voice by modifying `CARTESIA_VOICE_ID` in `.env`. Visit [Cartesia's voice library](https://cartesia.ai/voices) to browse available voices.

### Model Selection

- **STT Model**: `ink-whisper` (optimized for conversational AI with Hebrew support)
- **LLM Model**: `gpt-4o-mini` (fast, cost-effective, excellent Hebrew support)
- **TTS Model**: `sonic-multilingual` (natural-sounding multilingual voice)

### Language Configuration

Currently configured for Hebrew (`he`). To change language:
1. Update `language: 'he'` in `src/server.js` (STT and TTS)
2. Update system prompt in `src/server.js` to target language
3. Select appropriate voice from Cartesia's library

## ğŸ› Troubleshooting

### Microphone Access Denied

- Check browser permissions for microphone access
- Ensure you're using HTTPS (or localhost for testing)
- Try refreshing the page and granting permission again

### Connection Errors

- Verify all API keys are correctly set in `.env`
- Check that you have sufficient credits/quota on Cartesia and OpenAI accounts
- Ensure firewall isn't blocking WebSocket connections
- Check console for detailed error messages

### Audio Issues

- Check browser console for errors
- Verify audio format settings match (16kHz, PCM S16LE)
- Test your microphone in other applications
- Try using headphones to prevent echo/feedback
- Check audio visualizer to confirm audio capture

### High Latency

- Check "Performance Metrics" panel to identify bottleneck
- Verify internet connection stability
- Consider server location (Vercel deployment recommended for global low latency)
- Check API service status pages

### API Rate Limits

- **Cartesia STT**: $0.13/hour
- **OpenAI GPT-4o-mini**: $0.15/1M input tokens, $0.60/1M output tokens
- Monitor your usage on respective dashboards

## ğŸ“ Project Structure

### Current Structure (Node.js)
```
cartesia-conversation-demo/
â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ index.html          # Frontend UI with latency panel
â”‚   â””â”€â”€ app.js              # Client-side WebSocket logic
â”œâ”€â”€ src/
â”‚   â””â”€â”€ server.js           # Backend WebSocket server
â”œâ”€â”€ .env.example            # Environment variables template
â”œâ”€â”€ .env                    # Your configuration (not in git)
â”œâ”€â”€ package.json            # Dependencies and scripts
â”œâ”€â”€ README.md              # This file
â””â”€â”€ VERCEL_DEPLOYMENT_PLAN.md  # Migration plan
```

### Future Structure (Vite + Vercel)
```
cartesia-conversation-demo/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ conversation.ts     # Edge Function handler
â”‚   â””â”€â”€ health.ts          # Health check
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/        # React components
â”‚   â”œâ”€â”€ services/          # API clients
â”‚   â””â”€â”€ main.ts           # Entry point
â”œâ”€â”€ public/                # Static assets
â”œâ”€â”€ vercel.json           # Vercel configuration
â”œâ”€â”€ vite.config.ts        # Vite configuration
â””â”€â”€ package.json          # Dependencies
```

## ğŸ› ï¸ Technologies Used

### Current Stack
- **Node.js**: Backend runtime
- **Express**: Web server framework
- **ws**: WebSocket library for Node.js
- **OpenAI SDK**: Official OpenAI client
- **Web Audio API**: Browser audio capture and playback
- **WebSocket API**: Real-time bidirectional communication

### Future Stack (Vercel)
- **Vite**: Lightning-fast build tool
- **React**: Modern UI framework
- **TypeScript**: Type-safe development
- **Vercel Edge Functions**: Serverless computing at the edge
- **HTTP Streaming (SSE)**: Real-time server-to-client communication
- **Tailwind CSS**: Utility-first styling

## ğŸ¯ Performance Targets

| Metric | Target | Current |
|--------|--------|---------|
| STT Latency | <500ms | âœ… Optimized |
| LLM First Token | <300ms | âœ… Streaming enabled |
| TTS First Byte | <150ms | âœ… Context streaming |
| End-to-End | <2000ms | âœ… Aggressive chunking |

## ğŸš€ Deployment

### Current Deployment (Self-hosted)

1. Set up Node.js server
2. Configure environment variables
3. Run `npm start`
4. Open firewall for WebSocket connections

### Vercel Deployment (Coming Soon)

1. Install Vercel CLI: `npm i -g vercel`
2. Link project: `vercel link`
3. Add environment variables: `vercel env add`
4. Deploy: `vercel deploy --prod`

See `VERCEL_DEPLOYMENT_PLAN.md` for complete migration guide.

## ğŸ“ License

MIT

## ğŸ‘¨â€ğŸ’» Credits

Created by **aviz**

Powered by:
- [Cartesia.ai](https://cartesia.ai) - STT/TTS APIs (sonic-multilingual)
- [OpenAI](https://openai.com) - GPT-4o-mini LLM

## ğŸ†˜ Support

For issues related to:
- **Cartesia API**: [Cartesia Documentation](https://docs.cartesia.ai)
- **OpenAI API**: [OpenAI Documentation](https://platform.openai.com/docs)
- **This project**: Open an issue in the repository

## ğŸ”— Resources

- **GitHub Repository**: https://github.com/aviz85/cartesia-conversation-demo
- **Deployment Plan**: See `VERCEL_DEPLOYMENT_PLAN.md`
- **Quick Start**: See `QUICKSTART.md`
